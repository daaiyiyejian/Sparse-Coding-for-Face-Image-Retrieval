{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spams\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/LFW_DATA.pickle\", \"rb\") as f:\n",
    "    lfw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_learning(patch_feature, lambda1=1, dictionary_size=100, batchsize=100,\n",
    "                       posD=True):\n",
    "    # input shape (feature size, sample size)\n",
    "    X_patch = np.asfortranarray(patch_feature)\n",
    "    param = { 'K' : dictionary_size, # learns a dictionary with 400 elements\n",
    "             \"mode\":0,\n",
    "              'lambda1' : lambda1, 'numThreads' : -1,\n",
    "             \"batchsize\":batchsize,\n",
    "             'posD':posD\n",
    "            }\n",
    "    D = spams.trainDL_Memory(X_patch,**param)\n",
    "    return D\n",
    "\n",
    "def sparse_feature_coding(patch_feature, dictionary, lambda1=1, pos=True):\n",
    "    # lasso\n",
    "    param = {\n",
    "        'lambda1' : lambda1, # not more than 20 non-zeros coefficients\n",
    "        'numThreads' : -1, \n",
    "        'mode' : 0, # penalized formulation\n",
    "        'pos' : pos\n",
    "    } \n",
    "    X_patch = np.asfortranarray(patch_feature)\n",
    "    alpha = spams.lasso(X_patch, D = D, return_reg_path = False, **param)\n",
    "    dense_alpha = scipy.sparse.csr_matrix.todense(alpha)\n",
    "    return dense_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 4720)\n",
      "(13113, 59)\n",
      "80\n",
      "(13113, 59)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(lfw[\"database_feature\"])\n",
    "print(X.shape)\n",
    "X_patch = np.asfortranarray(X[:,:59]) # change type to Fortran array\n",
    "print(X_patch.shape)\n",
    "X_split = np.split(X,80, axis=1) # split to 80 patch\n",
    "print(len(X_split))\n",
    "print(X_split[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch: 10\n",
      "patch: 20\n",
      "patch: 30\n",
      "patch: 40\n",
      "patch: 50\n",
      "patch: 60\n",
      "patch: 70\n",
      "patch: 80\n",
      "sparse database shape: (13113, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter\n",
    "BATCHSIZE = 100\n",
    "DICTIONARY_SIZE = 100\n",
    "POS_D_CONSTRAINT =  True\n",
    "POS_LARS_CONSTRAINT =  True\n",
    "LAMBDA_DL = 1\n",
    "LAMBDA_LARS = 1\n",
    "\n",
    "\n",
    "# train 80 different dictionaries for all parts in the faces\n",
    "# Hyper-parameter\n",
    "BATCHSIZE = 100\n",
    "DICTIONARY_SIZE = 100\n",
    "POS_D_CONSTRAINT =  True\n",
    "POS_LARS_CONSTRAINT =  True\n",
    "LAMBDA_DL = 1\n",
    "LAMBDA_LARS = 1\n",
    "\n",
    "# train 80 different dictionaries for all parts in the faces\n",
    "D_list = []\n",
    "alpha_list = []\n",
    "patch = 0\n",
    "for single_patch in X_split:\n",
    "    patch+=1\n",
    "    X_patch = single_patch.T\n",
    "    if patch%10 == 0:\n",
    "        print(\"patch:\",patch)\n",
    "    # learn dictionary for single patch\n",
    "    D = dictionary_learning(X_patch,\n",
    "                            lambda1=LAMBDA_DL, \n",
    "                            dictionary_size=DICTIONARY_SIZE,\n",
    "                            batchsize=BATCHSIZE, \n",
    "                            posD=POS_D_CONSTRAINT)\n",
    "    alpha = sparse_feature_coding(X_patch, D, \n",
    "                                  lambda1=LAMBDA_LARS, \n",
    "                                  pos=POS_LARS_CONSTRAINT).T\n",
    "    D_list.append(D)\n",
    "    alpha_list.append(alpha)\n",
    "\n",
    "sparse_database_feature = np.concatenate(np.array(alpha_list),axis=1) \n",
    "print(\"sparse database shape:\",sparse_database_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch: 10 dictionary_index: 10\n",
      "patch: 20 dictionary_index: 20\n",
      "patch: 30 dictionary_index: 30\n",
      "patch: 40 dictionary_index: 40\n",
      "patch: 50 dictionary_index: 50\n",
      "patch: 60 dictionary_index: 60\n",
      "patch: 70 dictionary_index: 70\n",
      "patch: 80 dictionary_index: 80\n",
      "sparse query shape (120, 8000)\n"
     ]
    }
   ],
   "source": [
    "# sparse encode query feature\n",
    "X_query = lfw[\"query_feature\"]\n",
    "# train 80 different dictionaries for all parts in the faces\n",
    "alpha_list = []\n",
    "patch = 0\n",
    "dictionary_index = 0\n",
    "X_query_split = np.split(X_query,80, axis=1)\n",
    "for single_patch in X_query_split:\n",
    "    patch+=1\n",
    "    D = D_list[dictionary_index]\n",
    "    dictionary_index += 1\n",
    "    X_patch = np.asfortranarray(single_patch).T\n",
    "    if patch%10 == 0:\n",
    "        print(\"patch:\",patch, \"dictionary_index:\",dictionary_index)\n",
    "    # learn dictionary for single patch\n",
    "    \n",
    "    alpha = sparse_feature_coding(X_patch, D, lambda1=LAMBDA_LARS, pos=POS_LARS_CONSTRAINT).T\n",
    "    alpha_list.append(alpha)\n",
    "sparse_query_feature = np.concatenate(np.array(alpha_list),axis=1) \n",
    "print(\"sparse query shape\",sparse_query_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 5749 different names in database\n"
     ]
    }
   ],
   "source": [
    "# identity\n",
    "database_id_list = np.array([lfw[\"database_identity\"][i][0][0].split(\"\\\\\")[0] for i in range(len(lfw[\"database_identity\"]))])\n",
    "\n",
    "database_id_uni = np.array(sorted(list(set(database_id_list))))\n",
    "print(\"have {} different names in database\".format(len(database_id_uni)))\n",
    "sparse_database_feature_copy = np.copy(sparse_database_feature)\n",
    "weight = 5\n",
    "for identity in database_id_uni:\n",
    "    id_mask = database_id_list == identity\n",
    "    sub_feature = sparse_database_feature[id_mask]\n",
    "    id_mean_feature = sub_feature.mean(axis=0)\n",
    "    sparse_database_feature_copy[id_mask,:] = sparse_database_feature[id_mask,:]*weight + \\\n",
    "    id_mean_feature*(1-weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"../data/sparse_database_feature_identityF.npy\",sparse_database_feature_copy)\n",
    "np.save(\"../data/sparse_query_feature_identityF.npy\",sparse_query_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13113, 8000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sparse_database_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_database_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
